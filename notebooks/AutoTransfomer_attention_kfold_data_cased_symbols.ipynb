{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"AutoTransfomer_attention_kfold_data_cased_symbols.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"622400a84b3a48808bdc7c4410db6cb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bfdf1fc0498747329f8482de8897564a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_313eba77a6124687bda084352cab658a","IPY_MODEL_a85db098e7264932aa246f3dd68e8b75"]}},"bfdf1fc0498747329f8482de8897564a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"313eba77a6124687bda084352cab658a":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e8e546bb2d114230beb0ac8583638ad7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_318a023aa058437cad72ef84613aa9b7"}},"a85db098e7264932aa246f3dd68e8b75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_09953ab9abb4420c85a0a6a6b381103c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 361/361 [00:02&lt;00:00, 152B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38a3ceab7e1047f4a99113a699abfadd"}},"e8e546bb2d114230beb0ac8583638ad7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"318a023aa058437cad72ef84613aa9b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09953ab9abb4420c85a0a6a6b381103c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"38a3ceab7e1047f4a99113a699abfadd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a651c79740f144cb8b57588ec0e6d265":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b965f017863541718603f26f44276c0f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a69ae13afbe8497aa8f44a32f44e3767","IPY_MODEL_f63498a2e5ff4b8c8afd1645f1da144b"]}},"b965f017863541718603f26f44276c0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a69ae13afbe8497aa8f44a32f44e3767":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a3a03aedb0de4ef4b77dcb3280414017","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8be971b9cead4815928172a8a9cfed3a"}},"f63498a2e5ff4b8c8afd1645f1da144b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a3c923688a49477eac22a2ebb60e78ed","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 282kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a13896ccc55e41b3a0299baa0218442c"}},"a3a03aedb0de4ef4b77dcb3280414017":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8be971b9cead4815928172a8a9cfed3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a3c923688a49477eac22a2ebb60e78ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a13896ccc55e41b3a0299baa0218442c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"86ca76838ab44284b0c7dca4dcffca36":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d41e49cd30a5436ca264dad5dd2d3730","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_68b559829c234e919f0b10d486985a4d","IPY_MODEL_fff9d88732164497931cd20a81245d74"]}},"d41e49cd30a5436ca264dad5dd2d3730":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68b559829c234e919f0b10d486985a4d":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8eb2984a009445869a316b007ffd9906","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61ac26bd7e3f42439f6ce1fbdbb418e0"}},"fff9d88732164497931cd20a81245d74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_238ca8ef5cdb4036a020525530741050","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3276/? [00:05&lt;00:00, 642.77it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89b16c0377834510b2fd82f0d5f23ba9"}},"8eb2984a009445869a316b007ffd9906":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"61ac26bd7e3f42439f6ce1fbdbb418e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"238ca8ef5cdb4036a020525530741050":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"89b16c0377834510b2fd82f0d5f23ba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49e5b97c530d43faa92a0f5915da59a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a07bf6690d9440119d2d81d0d146f0f5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3643d55b0e3f4b0891ee95a4d3d180c0","IPY_MODEL_49f4b3f311584e28a1d66991c433b1d9"]}},"a07bf6690d9440119d2d81d0d146f0f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3643d55b0e3f4b0891ee95a4d3d180c0":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dabeed9ebe674261a2054cb7b89166b5","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c162b5ccab84915b93fa867d002748c"}},"49f4b3f311584e28a1d66991c433b1d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3b98924635114195a2664ad5c462f6b2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1404/? [00:04&lt;00:00, 310.43it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f22cf8318664127bde5870b0eab6616"}},"dabeed9ebe674261a2054cb7b89166b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2c162b5ccab84915b93fa867d002748c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b98924635114195a2664ad5c462f6b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f22cf8318664127bde5870b0eab6616":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"z6TuFfcvZyno","colab_type":"code","outputId":"4e258e05-2952-4b49-8eaa-832bcac508fa","executionInfo":{"status":"ok","timestamp":1586328703387,"user_tz":-330,"elapsed":27240,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LWFFLdrJZbNq","colab_type":"code","outputId":"62076877-2260-482c-99e3-913658710516","executionInfo":{"status":"ok","timestamp":1586328715030,"user_tz":-330,"elapsed":18327,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["! pip install transformers -q"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |▋                               | 10kB 31.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 864kB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 1.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 1.3MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 1.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 1.1MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 1.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 1.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 1.4MB/s \n","\u001b[?25h\u001b[?25l\r\u001b[K     |▎                               | 10kB 3.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 5.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 4.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 4.7MB/s \n","\u001b[K     |████████████████████████████████| 870kB 8.4MB/s \n","\u001b[K     |████████████████████████████████| 3.7MB 6.9MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9W8iuy5iZbN0","colab_type":"code","colab":{}},"source":["import csv\n","import pandas as pd\n","from pathlib import Path\n","import matplotlib.cm as cm\n","from fastai import *\n","from fastai.text import *\n","from fastai.callbacks import *\n","from fastai.metrics import *\n","import numpy as np\n","import pandas as pd\n","\n","from pathlib import Path\n","from typing import *\n","\n","import torch\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJUOCfwJ7nG7","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDcJAbWQmWmW","colab_type":"code","colab":{}},"source":["seed = 42\n","\n","# python RNG\n","import random\n","random.seed(seed)\n","\n","# pytorch RNGs\n","import torch\n","torch.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n","\n","# numpy RNG\n","import numpy as np\n","np.random.seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pfkl3wOmaRkt","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/gdrive/My Drive/DEFINITION EXTRACTION/Definition')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"17X4qG7RZbN-","colab_type":"code","colab":{}},"source":["model_name=\"bert-base-cased\"\n","#model_name=\"bert-base-uncased\"\n","#model_name=\"roberta-base\"\n","#model_name=\"xlnet-base-cased\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ad9AScmzi00","colab_type":"code","outputId":"d8602755-bb85-4f26-87fc-4df8d36a98c4","executionInfo":{"status":"ok","timestamp":1586328732044,"user_tz":-330,"elapsed":3693,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":147}},"source":["merged = pd.read_csv('WCL_cased_symbols.csv')\n","merged.head(2)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>Psentence</th>\n","      <th>Length</th>\n","      <th>Dep1</th>\n","      <th>Dep2</th>\n","      <th>DepLabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>A Pianist   is  a person who plays the piano.</td>\n","      <td>9</td>\n","      <td>Pianist is is is person person plays plays piano</td>\n","      <td>A Pianist person . a plays who piano the</td>\n","      <td>nsubj ROOT ROOT ROOT attr attr relcl relcl dobj</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Pingala has a sunlike nature and male energy.</td>\n","      <td>8</td>\n","      <td>has has has nature nature nature nature energy</td>\n","      <td>Pingala nature . a sunlike and energy male</td>\n","      <td>ROOT ROOT ROOT dobj dobj dobj dobj conj</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label  ...                                         DepLabel\n","0      1  ...  nsubj ROOT ROOT ROOT attr attr relcl relcl dobj\n","1      0  ...          ROOT ROOT ROOT dobj dobj dobj dobj conj\n","\n","[2 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"_Cggr4LVl1Le","colab_type":"code","colab":{}},"source":["def seq_len(row):\n","  return len(row['Psentence'].split())\n","\n","merged['Length'] = merged.apply(seq_len, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oj6xgU_Nl30d","colab_type":"code","outputId":"c91a1bcf-531e-4d23-b1c8-640872675f7b","executionInfo":{"status":"ok","timestamp":1586328732047,"user_tz":-330,"elapsed":844,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["merged = merged[merged['Length']>4]\n","merged.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4680, 6)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"ZI5qRserOi64","colab_type":"text"},"source":["## Transformer"]},{"cell_type":"code","metadata":{"id":"8TBbnX8TZbOF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":133,"referenced_widgets":["622400a84b3a48808bdc7c4410db6cb5","bfdf1fc0498747329f8482de8897564a","313eba77a6124687bda084352cab658a","a85db098e7264932aa246f3dd68e8b75","e8e546bb2d114230beb0ac8583638ad7","318a023aa058437cad72ef84613aa9b7","09953ab9abb4420c85a0a6a6b381103c","38a3ceab7e1047f4a99113a699abfadd","a651c79740f144cb8b57588ec0e6d265","b965f017863541718603f26f44276c0f","a69ae13afbe8497aa8f44a32f44e3767","f63498a2e5ff4b8c8afd1645f1da144b","a3a03aedb0de4ef4b77dcb3280414017","8be971b9cead4815928172a8a9cfed3a","a3c923688a49477eac22a2ebb60e78ed","a13896ccc55e41b3a0299baa0218442c"]},"outputId":"3fed9819-f006-41fa-f389-8db65d0b4570","executionInfo":{"status":"ok","timestamp":1586328745576,"user_tz":-330,"elapsed":10483,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}}},"source":[" %tensorflow_version 1.x \n","from transformers import BertTokenizer, GPT2Tokenizer, AutoTokenizer\n","from transformers import BertConfig, BertForSequenceClassification, AutoModel, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, output_attentions=True)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"622400a84b3a48808bdc7c4410db6cb5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a651c79740f144cb8b57588ec0e6d265","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_wid…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OSwjVTtbi72R","colab_type":"text"},"source":["## LM Training"]},{"cell_type":"code","metadata":{"id":"jlehrB5Ci-gU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["86ca76838ab44284b0c7dca4dcffca36","d41e49cd30a5436ca264dad5dd2d3730","68b559829c234e919f0b10d486985a4d","fff9d88732164497931cd20a81245d74","8eb2984a009445869a316b007ffd9906","61ac26bd7e3f42439f6ce1fbdbb418e0","238ca8ef5cdb4036a020525530741050","89b16c0377834510b2fd82f0d5f23ba9","49e5b97c530d43faa92a0f5915da59a3","a07bf6690d9440119d2d81d0d146f0f5","3643d55b0e3f4b0891ee95a4d3d180c0","49f4b3f311584e28a1d66991c433b1d9","dabeed9ebe674261a2054cb7b89166b5","2c162b5ccab84915b93fa867d002748c","3b98924635114195a2664ad5c462f6b2","4f22cf8318664127bde5870b0eab6616"]},"outputId":"863115f8-27c0-4ff6-b028-6d4c7fe81ad1","executionInfo":{"status":"ok","timestamp":1586329250787,"user_tz":-330,"elapsed":1850,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}}},"source":["from tqdm.notebook import tqdm\n","from sklearn.model_selection import train_test_split\n","\n","t,v  = train_test_split(merged, test_size=0.3)\n","\n","with open('deft.train.raw', 'w') as f:\n","  for i,row in tqdm(t.iterrows()):\n","    f.write(row['Psentence']+'\\n')\n","\n","with open('deft.test.raw', 'w') as f:\n","  for i,row in tqdm(v.iterrows()):\n","    f.write(row['Psentence']+'\\n')\n"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86ca76838ab44284b0c7dca4dcffca36","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49e5b97c530d43faa92a0f5915da59a3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UtVesqoxjD45","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f23257af-c982-4c50-ddb0-c73fa4df51f9","executionInfo":{"status":"ok","timestamp":1586329338062,"user_tz":-330,"elapsed":85365,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}}},"source":["! rm -rf bert_ft\n","! mkdir bert_ft\n","! python train_lm.py \\\n","    --output_dir=bert_ft \\\n","    --model_type=bert \\\n","    --model_name_or_path=bert-base-cased \\\n","    --do_train \\\n","    --train_data_file=./deft.train.raw \\\n","    --do_eval \\\n","    --eval_data_file=./deft.test.raw \\\n","    --mlm\\"],"execution_count":24,"outputs":[{"output_type":"stream","text":["04/08/2020 07:01:05 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","04/08/2020 07:01:05 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e\n","04/08/2020 07:01:05 - INFO - transformers.configuration_utils -   Model config BertConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": null,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": null,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 512,\n","  \"min_length\": 0,\n","  \"model_type\": \"bert\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 28996\n","}\n","\n","04/08/2020 07:01:06 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n","04/08/2020 07:01:07 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n","04/08/2020 07:01:11 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n","04/08/2020 07:01:11 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","04/08/2020 07:01:13 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='./deft.test.raw', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='bert-base-cased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='bert_ft', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='./deft.train.raw', warmup_steps=0, weight_decay=0.0)\n","04/08/2020 07:01:13 - INFO - __main__ -   Loading features from cached file ./bert_cached_lm_510_deft.train.raw\n","04/08/2020 07:01:13 - INFO - __main__ -   ***** Running training *****\n","04/08/2020 07:01:13 - INFO - __main__ -     Num examples = 197\n","04/08/2020 07:01:13 - INFO - __main__ -     Num Epochs = 1\n","04/08/2020 07:01:13 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n","04/08/2020 07:01:13 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n","04/08/2020 07:01:13 - INFO - __main__ -     Gradient Accumulation steps = 1\n","04/08/2020 07:01:13 - INFO - __main__ -     Total optimization steps = 50\n","Epoch:   0% 0/1 [00:00<?, ?it/s]\n","Iteration:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n","Iteration:   2% 1/50 [00:01<00:52,  1.07s/it]\u001b[A\n","Iteration:   4% 2/50 [00:02<00:49,  1.04s/it]\u001b[A\n","Iteration:   6% 3/50 [00:02<00:47,  1.02s/it]\u001b[A\n","Iteration:   8% 4/50 [00:03<00:45,  1.01it/s]\u001b[A\n","Iteration:  10% 5/50 [00:04<00:43,  1.03it/s]\u001b[A\n","Iteration:  12% 6/50 [00:05<00:42,  1.04it/s]\u001b[A\n","Iteration:  14% 7/50 [00:06<00:40,  1.05it/s]\u001b[A\n","Iteration:  16% 8/50 [00:07<00:39,  1.06it/s]\u001b[A\n","Iteration:  18% 9/50 [00:08<00:38,  1.06it/s]\u001b[A\n","Iteration:  20% 10/50 [00:09<00:37,  1.07it/s]\u001b[A\n","Iteration:  22% 11/50 [00:10<00:36,  1.07it/s]\u001b[A\n","Iteration:  24% 12/50 [00:11<00:35,  1.07it/s]\u001b[A\n","Iteration:  26% 13/50 [00:12<00:34,  1.07it/s]\u001b[A\n","Iteration:  28% 14/50 [00:13<00:33,  1.07it/s]\u001b[A\n","Iteration:  30% 15/50 [00:14<00:32,  1.07it/s]\u001b[A\n","Iteration:  32% 16/50 [00:15<00:31,  1.08it/s]\u001b[A\n","Iteration:  34% 17/50 [00:16<00:30,  1.07it/s]\u001b[A\n","Iteration:  36% 18/50 [00:16<00:29,  1.07it/s]\u001b[A\n","Iteration:  38% 19/50 [00:17<00:28,  1.07it/s]\u001b[A\n","Iteration:  40% 20/50 [00:18<00:27,  1.08it/s]\u001b[A\n","Iteration:  42% 21/50 [00:19<00:26,  1.08it/s]\u001b[A\n","Iteration:  44% 22/50 [00:20<00:25,  1.08it/s]\u001b[A\n","Iteration:  46% 23/50 [00:21<00:25,  1.08it/s]\u001b[A\n","Iteration:  48% 24/50 [00:22<00:24,  1.07it/s]\u001b[A\n","Iteration:  50% 25/50 [00:23<00:23,  1.08it/s]\u001b[A\n","Iteration:  52% 26/50 [00:24<00:22,  1.07it/s]\u001b[A\n","Iteration:  54% 27/50 [00:25<00:21,  1.08it/s]\u001b[A\n","Iteration:  56% 28/50 [00:26<00:20,  1.08it/s]\u001b[A\n","Iteration:  58% 29/50 [00:27<00:19,  1.08it/s]\u001b[A\n","Iteration:  60% 30/50 [00:28<00:18,  1.08it/s]\u001b[A\n","Iteration:  62% 31/50 [00:29<00:17,  1.07it/s]\u001b[A\n","Iteration:  64% 32/50 [00:29<00:16,  1.07it/s]\u001b[A\n","Iteration:  66% 33/50 [00:30<00:15,  1.08it/s]\u001b[A\n","Iteration:  68% 34/50 [00:31<00:14,  1.08it/s]\u001b[A\n","Iteration:  70% 35/50 [00:32<00:13,  1.08it/s]\u001b[A\n","Iteration:  72% 36/50 [00:33<00:12,  1.08it/s]\u001b[A\n","Iteration:  74% 37/50 [00:34<00:12,  1.07it/s]\u001b[A\n","Iteration:  76% 38/50 [00:35<00:11,  1.07it/s]\u001b[A\n","Iteration:  78% 39/50 [00:36<00:10,  1.07it/s]\u001b[A\n","Iteration:  80% 40/50 [00:37<00:09,  1.07it/s]\u001b[A\n","Iteration:  82% 41/50 [00:38<00:08,  1.07it/s]\u001b[A\n","Iteration:  84% 42/50 [00:39<00:07,  1.08it/s]\u001b[A\n","Iteration:  86% 43/50 [00:40<00:06,  1.08it/s]\u001b[A\n","Iteration:  88% 44/50 [00:41<00:05,  1.07it/s]\u001b[A\n","Iteration:  90% 45/50 [00:42<00:04,  1.07it/s]\u001b[A\n","Iteration:  92% 46/50 [00:42<00:03,  1.07it/s]\u001b[A\n","Iteration:  94% 47/50 [00:43<00:02,  1.07it/s]\u001b[A\n","Iteration:  96% 48/50 [00:44<00:01,  1.07it/s]\u001b[A\n","Iteration:  98% 49/50 [00:45<00:00,  1.07it/s]\u001b[A\n","Iteration: 100% 50/50 [00:46<00:00,  1.08it/s]\n","Epoch: 100% 1/1 [00:46<00:00, 46.11s/it]\n","04/08/2020 07:01:59 - INFO - __main__ -    global_step = 50, average loss = 3.587595224380493\n","04/08/2020 07:01:59 - INFO - __main__ -   Saving model checkpoint to bert_ft\n","04/08/2020 07:01:59 - INFO - transformers.configuration_utils -   Configuration saved in bert_ft/config.json\n","04/08/2020 07:02:01 - INFO - transformers.modeling_utils -   Model weights saved in bert_ft/pytorch_model.bin\n","04/08/2020 07:02:01 - INFO - transformers.configuration_utils -   loading configuration file bert_ft/config.json\n","04/08/2020 07:02:01 - INFO - transformers.configuration_utils -   Model config BertConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": null,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": null,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 512,\n","  \"min_length\": 0,\n","  \"model_type\": \"bert\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 28996\n","}\n","\n","04/08/2020 07:02:01 - INFO - transformers.modeling_utils -   loading weights file bert_ft/pytorch_model.bin\n","04/08/2020 07:02:05 - INFO - transformers.tokenization_utils -   Model name 'bert_ft' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert_ft' is a path, a model identifier, or url to a directory containing tokenizer files.\n","04/08/2020 07:02:05 - INFO - transformers.tokenization_utils -   Didn't find file bert_ft/added_tokens.json. We won't load it.\n","04/08/2020 07:02:05 - INFO - transformers.tokenization_utils -   loading file bert_ft/vocab.txt\n","04/08/2020 07:02:05 - INFO - transformers.tokenization_utils -   loading file None\n","04/08/2020 07:02:05 - INFO - transformers.tokenization_utils -   loading file bert_ft/special_tokens_map.json\n","04/08/2020 07:02:05 - INFO - transformers.tokenization_utils -   loading file bert_ft/tokenizer_config.json\n","04/08/2020 07:02:05 - INFO - __main__ -   Evaluate the following checkpoints: ['bert_ft']\n","04/08/2020 07:02:05 - INFO - transformers.configuration_utils -   loading configuration file bert_ft/config.json\n","04/08/2020 07:02:05 - INFO - transformers.configuration_utils -   Model config BertConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": null,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": null,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 512,\n","  \"min_length\": 0,\n","  \"model_type\": \"bert\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 28996\n","}\n","\n","04/08/2020 07:02:05 - INFO - transformers.modeling_utils -   loading weights file bert_ft/pytorch_model.bin\n","04/08/2020 07:02:10 - INFO - __main__ -   Loading features from cached file ./bert_cached_lm_510_deft.test.raw\n","04/08/2020 07:02:10 - INFO - __main__ -   ***** Running evaluation  *****\n","04/08/2020 07:02:10 - INFO - __main__ -     Num examples = 80\n","04/08/2020 07:02:10 - INFO - __main__ -     Batch size = 4\n","Evaluating: 100% 20/20 [00:06<00:00,  3.22it/s]\n","04/08/2020 07:02:16 - INFO - __main__ -   ***** Eval results  *****\n","04/08/2020 07:02:16 - INFO - __main__ -     perplexity = tensor(27.5601)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uqMNSbhMwCeA","colab_type":"text"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"EJtATahguhJR","colab_type":"code","outputId":"6887e2ad-539e-4016-aa4b-d102639ade4a","executionInfo":{"status":"ok","timestamp":1586328921466,"user_tz":-330,"elapsed":4085,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.preprocessing.sequence import pad_sequences\n","MAX_LEN = 64\n","X = [tokenizer.encode(x, add_special_tokens=True) for x in merged['Psentence']]\n","X = pad_sequences(X, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"g84o6yiZwfBr","colab_type":"code","colab":{}},"source":["attention_masks = []\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in X:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)\n","attention_masks = np.array(attention_masks)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oT3_hc6Sw0tw","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmQYwCcWxDQT","colab_type":"code","colab":{}},"source":["y = np.array(merged['label'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCrL_HHYxQwa","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"vs29Zck60rpB","colab_type":"code","outputId":"67691ad0-e99b-4220-80f2-e86360eaf06f","executionInfo":{"status":"ok","timestamp":1586328922127,"user_tz":-330,"elapsed":957,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n","device"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"9mqC5yTxyV3-","colab_type":"code","colab":{}},"source":["from sklearn.utils.extmath import softmax\n","from sklearn.metrics import classification_report, f1_score\n","\n","\n","def flat_accuracy(preds, labels):\n","    preds = softmax(preds)\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","def flat_f1(preds, labels):\n","    preds = softmax(preds)\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, pred_flat)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZww_KS_w93w","colab_type":"code","outputId":"fb6ed58e-87a3-443a-d3c3-46a719a2ae2f","executionInfo":{"status":"ok","timestamp":1586332771125,"user_tz":-330,"elapsed":3417024,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.model_selection import KFold,StratifiedKFold\n","from tqdm import tqdm\n","from transformers import *\n","\n","auto_model = None\n","f1_scores, p_scores, r_scores = [], [], []\n","\n","kf = StratifiedKFold(10, shuffle=True, random_state=2018)\n","for f,(tr, val) in enumerate(kf.split(X, y)):\n","    print(f'--------------------------------------------------------------------------------Fold # {f}--------------------------------------------------------------------')\n","\n","    X_train, X_val = X[tr], X[val]\n","    y_train, y_val = y[tr], y[val]\n","    X_train_masks, X_val_masks = attention_masks[tr], attention_masks[val]\n","                                         \n","    # Convert all of our data into torch tensors, the required datatype for our model\n","    X_train = torch.tensor(X_train)\n","    X_val = torch.tensor(X_val)\n","\n","    y_train = torch.tensor(y_train)\n","    y_val = torch.tensor(y_val)\n","\n","    X_train_masks = torch.tensor(X_train_masks)\n","    X_val_masks = torch.tensor(X_val_masks)\n","\n","    batch_size = 32\n","\n","    # Create an iterator of our data with torch DataLoader \n","    train_data = TensorDataset(X_train, X_train_masks, y_train)\n","    train_sampler = SequentialSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    validation_data = TensorDataset(X_val, X_val_masks, y_val)\n","    validation_sampler = SequentialSampler(validation_data)\n","    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","\n","    ## Model\n","    del auto_model\n","    # auto_model = AutoModelForSequenceClassification.from_pretrained('./bert_ft', num_labels= len(np.unique(y_train)))\n","    auto_model = BertForSequenceClassification.from_pretrained('./bert_ft', output_attentions=False, num_labels= len(np.unique(y_train)))\n","\n","    auto_model.to(device)\n","\n","    # Optimizer\n","    num_total_steps = 1000\n","    num_warmup_steps = 100\n","    lr = 2e-6\n","    param_optimizer = list(auto_model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.0}\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n","    \n","\n","    auto_model.train()  \n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    n_epochs = 4\n","\n","    for epoch in (range(n_epochs)):  \n","      # Training Loop\n","      for step, batch in (enumerate(train_dataloader)):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        optimizer.zero_grad()\n","        outputs = auto_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","\n","        loss, logits = outputs[:2]\n","        loss.backward()\n","        optimizer.step()\n","        \n","        tr_loss += loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","          \n","      auto_model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","\n","      # Validation Loop\n","      yt, yp = [], []\n","      for batch in validation_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","          outputs = auto_model(b_input_ids, \n","                              token_type_ids=None,\n","                              attention_mask=b_input_mask,\n","                              labels= b_labels)        \n","          loss, logits = outputs[:2]\n","        logits = logits.detach().cpu().numpy()\n","        \n","        preds = softmax(logits)\n","        pred_ids = np.argmax(preds, axis=1).flatten()\n","\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        eval_loss += loss.item()\n","        eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","        yt = yt + label_ids.tolist()\n","        yp = yp + pred_ids.tolist()\n","\n","        nb_eval_steps +=1\n","        \n","      print(\"Epoch {} | Train loss: {} | Validation Loss: {} \".format(epoch,\n","                                                                      tr_loss/nb_tr_steps,\n","                                                                      eval_loss/nb_eval_steps, \n","                                                                    ))\n","    #if epoch == n_epochs:\n","    z = classification_report(yt, yp, output_dict=True)\n","    f1_scores.append(z['1']['f1-score'])\n","    p_scores.append(z['1']['precision'])    \n","    r_scores.append(z['1']['recall'])\n","    \n","    print(classification_report(yt, yp))   \n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["--------------------------------------------------------------------------------Fold # 0--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.43265664306553925 | Validation Loss: 0.2864905774593353 \n","Epoch 1 | Train loss: 0.3117167953627579 | Validation Loss: 0.14560329417387644 \n","Epoch 2 | Train loss: 0.23800540099277942 | Validation Loss: 0.11043802698453267 \n","Epoch 3 | Train loss: 0.1920161129568111 | Validation Loss: 0.10155531018972397 \n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.97      0.97       280\n","           1       0.96      0.96      0.96       188\n","\n","    accuracy                           0.97       468\n","   macro avg       0.97      0.97      0.97       468\n","weighted avg       0.97      0.97      0.97       468\n","\n","--------------------------------------------------------------------------------Fold # 1--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.4136784520338882 | Validation Loss: 0.17905904948711396 \n","Epoch 1 | Train loss: 0.2773998033706889 | Validation Loss: 0.09566204398870468 \n","Epoch 2 | Train loss: 0.21120249190264279 | Validation Loss: 0.07069832012057305 \n","Epoch 3 | Train loss: 0.17168456537536148 | Validation Loss: 0.06480641501645247 \n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       280\n","           1       0.99      0.98      0.98       188\n","\n","    accuracy                           0.99       468\n","   macro avg       0.99      0.99      0.99       468\n","weighted avg       0.99      0.99      0.99       468\n","\n","--------------------------------------------------------------------------------Fold # 2--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.5347365993216182 | Validation Loss: 0.2008072684208552 \n","Epoch 1 | Train loss: 0.33695396402794303 | Validation Loss: 0.07254774247606595 \n","Epoch 2 | Train loss: 0.24891579173260717 | Validation Loss: 0.06047915245095889 \n","Epoch 3 | Train loss: 0.19824193345001814 | Validation Loss: 0.05649361809094747 \n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.98      0.99       281\n","           1       0.96      0.99      0.98       187\n","\n","    accuracy                           0.98       468\n","   macro avg       0.98      0.98      0.98       468\n","weighted avg       0.98      0.98      0.98       468\n","\n","--------------------------------------------------------------------------------Fold # 3--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.5758452618663962 | Validation Loss: 0.3057951509952545 \n","Epoch 1 | Train loss: 0.37615581929232134 | Validation Loss: 0.12319296871622404 \n","Epoch 2 | Train loss: 0.27675109140273896 | Validation Loss: 0.0788065254688263 \n","Epoch 3 | Train loss: 0.21944646065645484 | Validation Loss: 0.06783604808151722 \n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.99       281\n","           1       0.98      0.97      0.98       187\n","\n","    accuracy                           0.98       468\n","   macro avg       0.98      0.98      0.98       468\n","weighted avg       0.98      0.98      0.98       468\n","\n","--------------------------------------------------------------------------------Fold # 4--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.4184053938722972 | Validation Loss: 0.22848668297131855 \n","Epoch 1 | Train loss: 0.2810824044528558 | Validation Loss: 0.126646971454223 \n","Epoch 2 | Train loss: 0.21085315092344475 | Validation Loss: 0.11581906589368979 \n","Epoch 3 | Train loss: 0.16946299722467578 | Validation Loss: 0.1185968207816283 \n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.95      0.97       281\n","           1       0.93      0.98      0.96       187\n","\n","    accuracy                           0.97       468\n","   macro avg       0.96      0.97      0.96       468\n","weighted avg       0.97      0.97      0.97       468\n","\n","--------------------------------------------------------------------------------Fold # 5--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.49723077700896695 | Validation Loss: 0.2685601224501928 \n","Epoch 1 | Train loss: 0.33905163985874615 | Validation Loss: 0.14085605069994928 \n","Epoch 2 | Train loss: 0.2564571038537631 | Validation Loss: 0.11088375970721245 \n","Epoch 3 | Train loss: 0.20778368385226437 | Validation Loss: 0.12395269212623437 \n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.94      0.97       281\n","           1       0.92      0.98      0.95       187\n","\n","    accuracy                           0.96       468\n","   macro avg       0.95      0.96      0.96       468\n","weighted avg       0.96      0.96      0.96       468\n","\n","--------------------------------------------------------------------------------Fold # 6--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.4560412662950429 | Validation Loss: 0.280062198638916 \n","Epoch 1 | Train loss: 0.3129178098205364 | Validation Loss: 0.1307651696105798 \n","Epoch 2 | Train loss: 0.23628936961029817 | Validation Loss: 0.10733580663800239 \n","Epoch 3 | Train loss: 0.1906159211648628 | Validation Loss: 0.0946940578520298 \n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.97      0.97       281\n","           1       0.96      0.95      0.95       187\n","\n","    accuracy                           0.96       468\n","   macro avg       0.96      0.96      0.96       468\n","weighted avg       0.96      0.96      0.96       468\n","\n","--------------------------------------------------------------------------------Fold # 7--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.473618674571767 | Validation Loss: 0.2651447892189026 \n","Epoch 1 | Train loss: 0.3221348520565891 | Validation Loss: 0.14085046102603277 \n","Epoch 2 | Train loss: 0.2461770720064941 | Validation Loss: 0.11418867272635301 \n","Epoch 3 | Train loss: 0.200430123276557 | Validation Loss: 0.1079072235773007 \n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.98      0.97       281\n","           1       0.96      0.96      0.96       187\n","\n","    accuracy                           0.97       468\n","   macro avg       0.97      0.97      0.97       468\n","weighted avg       0.97      0.97      0.97       468\n","\n","--------------------------------------------------------------------------------Fold # 8--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.5737161433154886 | Validation Loss: 0.28659816881020866 \n","Epoch 1 | Train loss: 0.3700397669507021 | Validation Loss: 0.11797121316194534 \n","Epoch 2 | Train loss: 0.2731473468187632 | Validation Loss: 0.09192506000399589 \n","Epoch 3 | Train loss: 0.21761304325473524 | Validation Loss: 0.0804475847631693 \n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.98      0.98       281\n","           1       0.97      0.97      0.97       187\n","\n","    accuracy                           0.98       468\n","   macro avg       0.98      0.98      0.98       468\n","weighted avg       0.98      0.98      0.98       468\n","\n","--------------------------------------------------------------------------------Fold # 9--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.45063917417869426 | Validation Loss: 0.2504591713349024 \n","Epoch 1 | Train loss: 0.3099276509389959 | Validation Loss: 0.10889797707398732 \n","Epoch 2 | Train loss: 0.2363898851767634 | Validation Loss: 0.08928261126081148 \n","Epoch 3 | Train loss: 0.1920261988592701 | Validation Loss: 0.07815933302044868 \n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.98       281\n","           1       0.98      0.97      0.97       187\n","\n","    accuracy                           0.98       468\n","   macro avg       0.98      0.98      0.98       468\n","weighted avg       0.98      0.98      0.98       468\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nZKQqi3oENmC","colab_type":"code","outputId":"86f53486-b0f3-4a4a-fc3f-5bcd49993b10","executionInfo":{"status":"ok","timestamp":1586333337523,"user_tz":-330,"elapsed":1330,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["np.mean(np.array(p_scores)), np.mean(np.array(r_scores)), np.mean(np.array(f1_scores))"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.9624533024464617, 0.9727585618386619, 0.9673992321810259)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"wUn7RV3FS3tm","colab_type":"code","outputId":"4f76590d-c679-4512-f58c-ab07a6e1ae06","executionInfo":{"status":"ok","timestamp":1578042819267,"user_tz":-330,"elapsed":1409932,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.model_selection import KFold,StratifiedKFold\n","from tqdm import tqdm\n","\n","auto_model = None\n","f1_scores, p_scores, r_scores = [], [], []\n","\n","kf = StratifiedKFold(10, shuffle=True, random_state=2018)\n","for f,(tr, val) in enumerate(kf.split(X, y)):\n","    print(f'--------------------------------------------------------------------------------Fold # {f}--------------------------------------------------------------------')\n","\n","    X_train, X_val = X[tr], X[val]\n","    y_train, y_val = y[tr], y[val]\n","    X_train_masks, X_val_masks = attention_masks[tr], attention_masks[val]\n","                                         \n","    # Convert all of our data into torch tensors, the required datatype for our model\n","    X_train = torch.tensor(X_train)\n","    X_val = torch.tensor(X_val)\n","\n","    y_train = torch.tensor(y_train)\n","    y_val = torch.tensor(y_val)\n","\n","    X_train_masks = torch.tensor(X_train_masks)\n","    X_val_masks = torch.tensor(X_val_masks)\n","\n","    batch_size = 32\n","\n","    # Create an iterator of our data with torch DataLoader \n","    train_data = TensorDataset(X_train, X_train_masks, y_train)\n","    train_sampler = SequentialSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    validation_data = TensorDataset(X_val, X_val_masks, y_val)\n","    validation_sampler = SequentialSampler(validation_data)\n","    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","\n","    ## Model\n","    del auto_model\n","    auto_model = AutoModelForSequenceClassification.from_pretrained(model_name, output_attentions=True, num_labels= len(np.unique(y_train)))\n","    auto_model.to(device)\n","\n","    # Optimizer\n","    num_total_steps = 1000\n","    num_warmup_steps = 100\n","    lr = 5e-6\n","    param_optimizer = list(auto_model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.0}\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n","    \n","\n","    auto_model.train()  \n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    n_epochs = 4\n","\n","    for epoch in (range(n_epochs)):  \n","      # Training Loop\n","      for step, batch in (enumerate(train_dataloader)):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        optimizer.zero_grad()\n","        outputs = auto_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","\n","        loss, logits = outputs[:2]\n","        loss.backward()\n","        optimizer.step()\n","        \n","        tr_loss += loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","          \n","      auto_model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","\n","      # Validation Loop\n","      yt, yp = [], []\n","      for batch in validation_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","          outputs = auto_model(b_input_ids, \n","                              token_type_ids=None,\n","                              attention_mask=b_input_mask,\n","                              labels= b_labels)        \n","          loss, logits = outputs[:2]\n","        logits = logits.detach().cpu().numpy()\n","        \n","        preds = softmax(logits)\n","        pred_ids = np.argmax(preds, axis=1).flatten()\n","\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        eval_loss += loss.item()\n","        eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","        yt = yt + label_ids.tolist()\n","        yp = yp + pred_ids.tolist()\n","\n","        nb_eval_steps +=1\n","        \n","      print(\"Epoch {} | Train loss: {} | Validation Loss: {} \".format(epoch,\n","                                                                      tr_loss/nb_tr_steps,\n","                                                                      eval_loss/nb_eval_steps, \n","                                                                    ))\n","    #if epoch == n_epochs:\n","    z = classification_report(yt, yp, output_dict=True)\n","    f1_scores.append(z['1']['f1-score'])\n","    p_scores.append(z['1']['precision'])    \n","    r_scores.append(z['1']['recall'])\n","    \n","    print(classification_report(yt, yp))   \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--------------------------------------------------------------------------------Fold # 0--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.300029432965499 | Validation Loss: 0.13655933489402136 \n","Epoch 1 | Train loss: 0.18946483818757714 | Validation Loss: 0.10540555492043495 \n","Epoch 2 | Train loss: 0.14054055631188958 | Validation Loss: 0.07922371911505859 \n","Epoch 3 | Train loss: 0.11196969468746278 | Validation Loss: 0.1049208715558052 \n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.99      0.98       281\n","           1       0.99      0.94      0.96       188\n","\n","    accuracy                           0.97       469\n","   macro avg       0.98      0.97      0.97       469\n","weighted avg       0.97      0.97      0.97       469\n","\n","--------------------------------------------------------------------------------Fold # 1--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.35085469089222676 | Validation Loss: 0.06669150268038114 \n","Epoch 1 | Train loss: 0.216722898562721 | Validation Loss: 0.036760372668504716 \n","Epoch 2 | Train loss: 0.15708698702839702 | Validation Loss: 0.03076415645579497 \n","Epoch 3 | Train loss: 0.12334280502435638 | Validation Loss: 0.022035967434446017 \n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       281\n","           1       0.99      0.99      0.99       188\n","\n","    accuracy                           0.99       469\n","   macro avg       0.99      0.99      0.99       469\n","weighted avg       0.99      0.99      0.99       469\n","\n","--------------------------------------------------------------------------------Fold # 2--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.30711521290129784 | Validation Loss: 0.13106636504332225 \n","Epoch 1 | Train loss: 0.19424279675480316 | Validation Loss: 0.09076243874927362 \n","Epoch 2 | Train loss: 0.14321539813012937 | Validation Loss: 0.09070021888862054 \n","Epoch 3 | Train loss: 0.11364470769573624 | Validation Loss: 0.09413487054407596 \n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.97      0.98       281\n","           1       0.96      0.98      0.97       187\n","\n","    accuracy                           0.97       468\n","   macro avg       0.97      0.98      0.97       468\n","weighted avg       0.97      0.97      0.97       468\n","\n","--------------------------------------------------------------------------------Fold # 3--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.3070029793589404 | Validation Loss: 0.10471512873967488 \n","Epoch 1 | Train loss: 0.18996432551295694 | Validation Loss: 0.07560392220815022 \n","Epoch 2 | Train loss: 0.13982049579440495 | Validation Loss: 0.07268588083485762 \n","Epoch 3 | Train loss: 0.11144604530206627 | Validation Loss: 0.08093120058377584 \n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.98      0.99       281\n","           1       0.97      0.98      0.98       187\n","\n","    accuracy                           0.98       468\n","   macro avg       0.98      0.98      0.98       468\n","weighted avg       0.98      0.98      0.98       468\n","\n","--------------------------------------------------------------------------------Fold # 4--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.3180380088813377 | Validation Loss: 0.08743565107385318 \n","Epoch 1 | Train loss: 0.19453721172811295 | Validation Loss: 0.055649642397960025 \n","Epoch 2 | Train loss: 0.1415254791982171 | Validation Loss: 0.047725792850057284 \n","Epoch 3 | Train loss: 0.11085356979465084 | Validation Loss: 0.04939663877400259 \n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.99       281\n","           1       0.99      0.97      0.98       187\n","\n","    accuracy                           0.99       468\n","   macro avg       0.99      0.98      0.98       468\n","weighted avg       0.99      0.99      0.99       468\n","\n","--------------------------------------------------------------------------------Fold # 5--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.3119724422241702 | Validation Loss: 0.138495351622502 \n","Epoch 1 | Train loss: 0.19711637796100343 | Validation Loss: 0.11342704047759374 \n","Epoch 2 | Train loss: 0.14539588169627493 | Validation Loss: 0.12085920323928197 \n","Epoch 3 | Train loss: 0.11573748975889898 | Validation Loss: 0.111208987981081 \n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.96      0.97       281\n","           1       0.95      0.98      0.96       187\n","\n","    accuracy                           0.97       468\n","   macro avg       0.97      0.97      0.97       468\n","weighted avg       0.97      0.97      0.97       468\n","\n","--------------------------------------------------------------------------------Fold # 6--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.3212412509954337 | Validation Loss: 0.11868783806761106 \n","Epoch 1 | Train loss: 0.20109262772497127 | Validation Loss: 0.0838143157462279 \n","Epoch 2 | Train loss: 0.14775989614097834 | Validation Loss: 0.08353975315888723 \n","Epoch 3 | Train loss: 0.11590140207695324 | Validation Loss: 0.0995893092205127 \n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.98      0.98       281\n","           1       0.97      0.97      0.97       187\n","\n","    accuracy                           0.98       468\n","   macro avg       0.98      0.98      0.98       468\n","weighted avg       0.98      0.98      0.98       468\n","\n","--------------------------------------------------------------------------------Fold # 7--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.34447691392040614 | Validation Loss: 0.1221347615122795 \n","Epoch 1 | Train loss: 0.21515173187733375 | Validation Loss: 0.07867517483731111 \n","Epoch 2 | Train loss: 0.15761087716303088 | Validation Loss: 0.07148154601454734 \n","Epoch 3 | Train loss: 0.12425452836693941 | Validation Loss: 0.07400053335974614 \n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.98       281\n","           1       0.98      0.97      0.98       187\n","\n","    accuracy                           0.98       468\n","   macro avg       0.98      0.98      0.98       468\n","weighted avg       0.98      0.98      0.98       468\n","\n","--------------------------------------------------------------------------------Fold # 8--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.3112227364132802 | Validation Loss: 0.10913325101137161 \n","Epoch 1 | Train loss: 0.19512250364024306 | Validation Loss: 0.06941317928334077 \n","Epoch 2 | Train loss: 0.14385497159409252 | Validation Loss: 0.07406761062641938 \n","Epoch 3 | Train loss: 0.11499821996425674 | Validation Loss: 0.10387641054888566 \n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.95      0.97       280\n","           1       0.93      0.99      0.96       187\n","\n","    accuracy                           0.97       467\n","   macro avg       0.97      0.97      0.97       467\n","weighted avg       0.97      0.97      0.97       467\n","\n","--------------------------------------------------------------------------------Fold # 9--------------------------------------------------------------------\n","Epoch 0 | Train loss: 0.3093170423455762 | Validation Loss: 0.12048460158209005 \n","Epoch 1 | Train loss: 0.18948599789644394 | Validation Loss: 0.0909742459654808 \n","Epoch 2 | Train loss: 0.13740520509585474 | Validation Loss: 0.09995313934050501 \n","Epoch 3 | Train loss: 0.10691000423596989 | Validation Loss: 0.11403367728150139 \n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.97      0.97       280\n","           1       0.96      0.96      0.96       187\n","\n","    accuracy                           0.97       467\n","   macro avg       0.97      0.97      0.97       467\n","weighted avg       0.97      0.97      0.97       467\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m6RAZsZgacjf","colab_type":"code","outputId":"5ee73fe8-bf85-402d-9cc3-ffc99e155e87","executionInfo":{"status":"ok","timestamp":1578043041554,"user_tz":-330,"elapsed":2691,"user":{"displayName":"Priyanshu Kumar","photoUrl":"","userId":"03532460980266287093"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["np.mean(np.array(p_scores)), np.mean(np.array(r_scores)), np.mean(np.array(f1_scores))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.9696186608152031, 0.9743685288428718, 0.971791202294806)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_N3pLdh6qr6L","colab":{}},"source":["torch.save(auto_model.state_dict(), 'models/bert_cased_sym_attn_97_10fold.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G-KIjKKnqrBU","colab":{}},"source":["# auto_model.load_state_dict(torch.load('models/bert_new.pth'))\n","# print(f'Done')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_beEt5mfXue","colab_type":"text"},"source":["# Analysis of model"]},{"cell_type":"code","metadata":{"id":"EZ_68nmOUNVU","colab_type":"code","colab":{}},"source":["y_pred = []\n","y_true = []\n","\n","x = []\n","\n","for batch in tqdm(validation_dataloader):\n","  \n","  batch = tuple(t.to(device) for t in batch)\n","  b_input_ids, b_input_mask, b_labels = batch\n","  with torch.no_grad():\n","    outputs = auto_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n","    logits = outputs[0]\n","\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  prob = softmax(logits)\n","  pred = np.argmax(prob, axis=1)\n","  for i in b_input_ids:\n","    z = []\n","    for j in i:\n","      z.append(j.item())\n","    x.append(z)\n","  [y_pred.append(i) for i in prob[:,1]]\n","  [y_true.append(i) for i in label_ids]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qvWPxyffOrX","colab_type":"code","colab":{}},"source":["df = pd.DataFrame({'Prob':y_pred, 'class':y_true, 'input':x})\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"scxdXvxWiUUE","colab_type":"code","colab":{}},"source":["#tok1.decode(df.loc[1,'input'])\n","\n","df['sentence'] = df['input'].apply(lambda x:tok1.decode(x))\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTEFnauFfjIU","colab_type":"code","colab":{}},"source":["df['Correct'] = ((df['Prob']>=0.5)*1 == df['class'])*1\n","w = df[df['Correct']==0].copy()\n","w.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u1uAodpWfn30","colab_type":"code","colab":{}},"source":["w.sort_values('Prob', inplace=True)\n","w.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEjp4fnLfrsF","colab_type":"code","colab":{}},"source":["fp = w[w['class']==0].reset_index(drop=True)\n","fn = w[w['class']==1].reset_index(drop=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FO2WNg60mfnu","colab_type":"code","colab":{}},"source":["fp.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqmaZYjHmOCY","colab_type":"code","colab":{}},"source":["for i in range(10):\n","  print(fp.loc[i,'sentence'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttaXoFKjmu7v","colab_type":"code","colab":{}},"source":["for i in range(10):\n","  print(fn.loc[i,'sentence'])"],"execution_count":0,"outputs":[]}]}